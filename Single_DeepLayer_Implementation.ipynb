{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Calculating Test Values...\n",
      "Weight 1: [[ 1.74876917  0.69037531  2.39750037  4.22074372  2.84367052 -3.2793029 ]\n",
      " [ 0.94607663  2.95984354 -1.16858068  1.84487069 -4.98383001  1.70450212]\n",
      " [ 0.75977931 -1.4332936  -2.70919338  1.83731361  1.59713677  0.32270697]]\n",
      "B1 value: [[ 2.03825438]\n",
      " [-2.35786053]\n",
      " [ 1.84455744]\n",
      " [-1.58342913]\n",
      " [-1.04757252]\n",
      " [-0.70771107]]\n",
      "Weight 2: [[ 0.4064997 ]\n",
      " [-3.83561108]\n",
      " [-4.12922378]\n",
      " [ 4.77561435]\n",
      " [ 4.31632089]\n",
      " [ 1.32563674]]\n",
      "B2 value: [[0.13963662]]\n",
      "\n",
      "Error: 0.03709094399703563%\n",
      "Output: [[0.9999597  0.99976644 0.99981893 0.30398662]]\n",
      "[[1.    1.    1.    0.304]]\n",
      "\n",
      "Duration 144.15526390075684ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# fire \n",
    "# news\n",
    "# NLP -> \n",
    "\n",
    "import numpy as np\n",
    "def relu(x, deriv = False):\n",
    "    #ReLu logistic compaction\n",
    "    if(x > 0):\n",
    "        if(deriv): return 1\n",
    "        else: return x\n",
    "    else:\n",
    "        return 0\n",
    "def sigmoid(x, deriv = False):\n",
    "    # Sigmoid logistic compaction\n",
    "    if (deriv):\n",
    "        sig = sigmoid(x, False)\n",
    "        return sig * (1 - sig)\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "def tanh(x, deriv = False):\n",
    "    # Tanh logistic compaction\n",
    "    if(deriv):\n",
    "        return 1 - np.power(tanh(x, False), 2)\n",
    "    else:\n",
    "        return (1 - np.exp(-2*x))/(1 + np.exp(-2*x))\n",
    "\n",
    "def forward_propogate(X, W1, b1, W2, b2):\n",
    "    #Forward Propogation\n",
    "    Z1 = np.dot(W1.T, X) + b1\n",
    "    A1 = tanh(Z1, False)\n",
    "    \n",
    "    Z2 = np.dot(W2.T, A1) + b2;\n",
    "    A2 = sigmoid(Z2, False)\n",
    "    return (A1, A2, Z1, Z2)\n",
    "\n",
    "def propogate(X, W1, b1, W2, b2):\n",
    "    return forward_propogate(X, W1, b1, W2, b2)[1];\n",
    "\n",
    "def error(A, Y):\n",
    "    # Error function\n",
    "    return -(Y*np.log(A) + (1 - Y)*np.log(1 -A))\n",
    "\n",
    "def train(X, Y, N, act, seed, steps, l_rate):\n",
    "    m = Y.shape[1]   # num training examples\n",
    "    # N[0] = nx\n",
    "    # N[1] = numNeurons layer1\n",
    "    # column is 1 training example, with each row being another neuron for each layer\n",
    "    \n",
    "    # Setup Seed\n",
    "    np.random.seed(seed)\n",
    "    #N[0] is A0, A1 is N[1]\n",
    "    # Each column is a neuron, each row is a weight for previous row\n",
    "    W1 = np.random.randn(N[0], N[1])\n",
    "    W2 = np.random.randn(N[1],N[2])\n",
    "    \n",
    "    # Not necessary for b value to init non zero\n",
    "    b1 = np.random.randn(N[1],1) \n",
    "    b2 = np.random.randn(N[2],1)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        # Determine actual values, and find error compared to the predicted values\n",
    "        (A1, A2, Z1, Z2) = forward_propogate(X, W1, b1, W2, b2)\n",
    "        #print(A1.shape) # double check\n",
    "        J = np.average(error(A2, Y)) #mean error\n",
    "        \n",
    "        # Start Backpropogation\n",
    "        # Compute Derivatives\n",
    "        #dZ = A - Y\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = 1/m * np.dot(dZ2, A1.T).T  # dividing by m kinda unnecessary # N[1]xN[2] dim\n",
    "        dB2 = 1/m * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "        \n",
    "        #print(dZ2)\n",
    "        #print(np.dot(W2.T, dZ2)) #THE BUGGER\n",
    "        #print(np.dot(W2, dZ2)) #FIXED\n",
    "        \n",
    "        dZ1 = np.dot(W2, dZ2) * tanh(Z1, True) # elementwise multiplication of dot and derivative\n",
    "        \n",
    "        dW1 = 1/m * np.dot(dZ1, X.T).T  # dividing by m kinda unnecessary\n",
    "        dB1 = 1/m * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "        \n",
    "        # Step the variables according to the slopes\n",
    "        #print(W2)\n",
    "        #print(dW2)\n",
    "        W2 = W2 - l_rate * dW2\n",
    "        b2 = b2 - l_rate * dB2\n",
    "        \n",
    "        W1 = W1 - l_rate * dW1\n",
    "        b1 = b1 - l_rate * dB1\n",
    "        \n",
    "    return (W1, b1, W2, b2, J)\n",
    "\n",
    "N = np.array([3, 6, 1]) #size of NN, 1st is input layer, 2nd is the hidden layer, 3rd is the output layer \n",
    "act = 2 # temp,in real this would store the activation functions, in this example its hard coded\n",
    "iterations = 500\n",
    "learning_rate = 5 #step size scalar\n",
    "seed = 0 #Deterministic Seed \n",
    "# Training examples\n",
    "\n",
    "# 2x2 adder\n",
    "X = np.array([ # First 2 plus second 2\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 0, 1, 1],\n",
    "    [0, 0, 1, 0]\n",
    "]) \n",
    "\n",
    "# Predicted Values\n",
    "Y = np.array([ # 1st out 2nd out, carry out\n",
    "    [0, 1, 1, 0]\n",
    "    \n",
    "]) \n",
    "\n",
    "# Testing data\n",
    "test = np.array([\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 0, 1, 1],\n",
    "    [1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "tic = time.time() #Start timer\n",
    "# Train to find weights, and b value\n",
    "print(\"Training...\")\n",
    "(W1, b1, W2, b2, J) = train(X, Y, N, act, seed, iterations, learning_rate)\n",
    "\n",
    "toc = time.time()\n",
    "np.set_printoptions(suppress=True) # Ensure data is in decimal format\n",
    "print(\"Calculating Test Values...\")\n",
    "\n",
    "# Print values\n",
    "print(\"Weight 1: \", end = \"\")\n",
    "print(W1)\n",
    "print(\"B1 value: \", end = \"\")\n",
    "print(b1)\n",
    "\n",
    "print(\"Weight 2: \", end = \"\")\n",
    "print(W2)\n",
    "print(\"B2 value: \", end = \"\")\n",
    "print(b2)\n",
    "print()\n",
    "\n",
    "print(\"Error: \" + str(J * 100) + \"%\")\n",
    "print(\"Output: \", end = \"\")\n",
    "output = propogate(test, W1, b1, W2, b2) # use testing data\n",
    "\n",
    "print(output) # Use testing data\n",
    "print(np.round(output, 3))\n",
    "print(\"\\nDuration \" + str(1000 * (toc - tic)) + \"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Neural Network to solve Rubiks Cube!!!!!!!!!!\n",
    "\n",
    "# What is Prolog\n",
    "\n",
    "\n",
    "# IMPLEMENT 15 puzzle slide, when you learn Recurrent Neuaral Networks\n",
    "# Initial State: [1,6,3,8,10,2,7,15,13,5,4,0,9,14,11,12]\n",
    "# Goal State   : [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,0]\n",
    "# 0 is empty\n",
    "# Operations: UP DOWN LEFT RIGHT if: vacant spot adjacent\n",
    "# Legal Moves Calculator: (Assuming 1D Array)\n",
    "#     LEFT: IF (pos(0) % 4 != 3), the tile right to it\n",
    "#    RIGHT: IF (pos(0) % 4 != 0), the tile left to it\n",
    "#     DOWN: IF (pos(0) > 3), the tile 4 to the left of it\n",
    "#       UP: IF (pos(0) < 12), the tile 4 to the right of it\n",
    "\n",
    "# Manhattan Distance (RAW STEPS AWAY FOR EACH TILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
